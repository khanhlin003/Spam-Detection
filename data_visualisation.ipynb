{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73d4375",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04cd9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('fivethirtyeight') \n",
    "from wordcloud import WordCloud\n",
    "import enchant\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2123cb30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>ect</th>\n",
       "      <th>and</th>\n",
       "      <th>for</th>\n",
       "      <th>of</th>\n",
       "      <th>a</th>\n",
       "      <th>you</th>\n",
       "      <th>hou</th>\n",
       "      <th>in</th>\n",
       "      <th>...</th>\n",
       "      <th>connevey</th>\n",
       "      <th>jay</th>\n",
       "      <th>valued</th>\n",
       "      <th>lay</th>\n",
       "      <th>infrastructure</th>\n",
       "      <th>military</th>\n",
       "      <th>allowing</th>\n",
       "      <th>ff</th>\n",
       "      <th>dry</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   the  to  ect  and  for  of    a  you  hou  in  ...  connevey  jay  valued  \\\n",
       "0    0   0    1    0    0   0    2    0    0   0  ...         0    0       0   \n",
       "1    8  13   24    6    6   2  102    1   27  18  ...         0    0       0   \n",
       "2    0   0    1    0    0   0    8    0    0   4  ...         0    0       0   \n",
       "3    0   5   22    0    5   1   51    2   10   1  ...         0    0       0   \n",
       "4    7   6   17    1    5   2   57    0    9   3  ...         0    0       0   \n",
       "\n",
       "   lay  infrastructure  military  allowing  ff  dry  Label  \n",
       "0    0               0         0         0   0    0      0  \n",
       "1    0               0         0         0   1    0      0  \n",
       "2    0               0         0         0   0    0      0  \n",
       "3    0               0         0         0   0    0      0  \n",
       "4    0               0         0         0   1    0      0  \n",
       "\n",
       "[5 rows x 3001 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned-dataset.csv\")\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7537d",
   "metadata": {},
   "source": [
    "# Exploration data analysis\n",
    "\n",
    "## 1. Correlation between words and email labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation coefficients between each word and the target variable\n",
    "correlations = df.corr()['Label'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 words most strongly associated with spam emails\n",
    "top_spam = correlations[1:].tail(10)\n",
    "\n",
    "print(\"Top 10 words most strongly associated with spam emails:\")\n",
    "print(top_spam)\n",
    "\n",
    "# Top 10 words most strongly associated with not-spam emails\n",
    "top_not_spam = correlations[1:].head(10)\n",
    "print(\"Top 10 words most strongly associated with not-spam emails:\")\n",
    "print(top_not_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8524e1",
   "metadata": {},
   "source": [
    "The top 10 words most strongly associated with spam emails, in order of highest correlation coefficient, are: more, our, able, best, ur, sex, sec, money, and soft. The strongest word association with the spam label is the word \"more\", followed closely by \"our\" and \"able\". These words may be common in spam emails because they are often used to make false claims or offer something of value in exchange for personal information or money.\n",
    "\n",
    "On the other hand, the top 10 words most strongly associated with not-spam emails, in order of highest negative correlation coefficient, are: hpl, hanks, thank, attached, daren, forwarded, subject, hp, aren, and nom. The strongest word association with the not-spam label is the word \"hpl\", followed by \"hanks\" and \"thank\". These words may be common in not-spam emails because they are often used in professional or formal communications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1e6c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create separate dataframes for spam and non-spam emails\n",
    "spam_df = df[df['Label'] == 1][top_spam.index]\n",
    "not_spam_df = df[df['Label'] == 0][top_not_spam.index]\n",
    "\n",
    "# Calculate correlation matrices for spam and non-spam emails\n",
    "spam_corr = spam_df.corr()\n",
    "not_spam_corr = not_spam_df.corr()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 8))\n",
    "sb.heatmap(spam_corr, cmap=\"coolwarm\", annot=True, fmt='.2f', ax=ax1)\n",
    "ax1.set_title(\"Correlation Matrix for Top 10 Words in Spam Emails\")\n",
    "sb.heatmap(not_spam_corr, cmap=\"coolwarm\", annot=True, fmt='.2f', ax=ax2)\n",
    "ax2.set_title(\"Correlation Matrix for Top 10 Words in Non-Spam Emails\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26e6b9",
   "metadata": {},
   "source": [
    "Analyzing the correlation matrices for the top 10 words in spam and non-spam emails, we can observe some interesting patterns.\n",
    "\n",
    "In the spam email matrix, we can see that some words are highly correlated. For instance, `our` and `ur` have a correlation coefficient of 0.76, followed by `ur` and `sec` with 0.67. Additionally, `able` and `ur` have a correlation of 0.50, and `money` and `our` have a correlation of 0.40. Surprisingly, `sex` has almost no correlation with other words except for a slight negative correlation with `money` and `sec`.\n",
    "\n",
    "On the other hand, the non-spam email matrix shows much higher correlations between words. Some of these correlations are expected, such as `hp` and `hpl` with a coefficient of 0.99, `hanks` and `thanks` with 0.83, and `forwarded` and `subject` with 0.73. These findings indicate that non-spam emails tend to use words in a more structured and cohesive manner compared to spam emails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c599e",
   "metadata": {},
   "source": [
    "##  2. Distribution of label classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_count = (df['Label'] == 1).sum()\n",
    "not_spam_count = (df['Label'] == 0).sum()\n",
    "\n",
    "plt.bar(['Spam', 'Not Spam'], [spam_count, not_spam_count])\n",
    "plt.title('Distribution of email labels')\n",
    "for i, v in enumerate([spam_count, not_spam_count]):\n",
    "    plt.text(i, v, str(v), color='black', ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9edfd12",
   "metadata": {},
   "source": [
    "In this dataset, there is 3672 non-spam emaild and 1500 spam emails. This indicates that the dataset is imbalanced because the number of non-spam emails is significantly higher than the number of spam emails. Therefore, the model may have better performance on non-spam emails but may not perform well on spam emails.Furthermore,an imbalanced data may lead to more incorrect predictions as the model may have a bias towards the majority class and may not be able to capture the patterns and relationships in the minority class.\n",
    "\n",
    "In the end, we have used several techniques to solve this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9506b",
   "metadata": {},
   "source": [
    "## 3. Word frequency in spam and not-spam emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636fbee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = df.loc[df['Label'] == 1].iloc[:, 1:-1].sum().sort_values(ascending=False)[:10]\n",
    "print(\"\\nTop 10 words with highest frequency in spam emails:\")\n",
    "for word, count in spam_words.items():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "not_spam_words = df.loc[df['Label'] == 0].iloc[:, 1:-1].sum().sort_values(ascending=False)[:10]\n",
    "print(\"\\nTop 10 words with highest frequency in non-spam emails:\")\n",
    "for word, count in not_spam_words.items():\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84650e6",
   "metadata": {},
   "source": [
    "The frequency of the top 10 most common words in both spam and not-spam emails were also calculated. The results show that the most common words in both types of emails are similar, with \"e\", \"t\", \"a\", \"o\", \"n\", \"r\", \"i\", \"s\", \"l\", and \"c\" being the top 10 in both cases, though their frequencies are higher in not-spam emails.\n",
    "\n",
    "It is worth noting that while the frequency of these common words may not be particularly informative in distinguishing between spam and not-spam emails on their own, they could be useful in combination with other features in a machine learning model. For example, the presence or absence of these common words could be used in combination with the top 10 words most strongly associated with either label to improve the accuracy of email classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249defc",
   "metadata": {},
   "source": [
    " Instead of just letters,lets try to words with the highest frequency in spam and non spam emails to have a more meaningful visualisation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b291c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern = r'^[A-Za-z]{3,}$' # Regular expression pattern for single letter column names\n",
    "filtered_df = df.filter(regex=pattern)\n",
    "d = enchant.Dict(\"en_US\") # Create an instance of the PyEnchant English dictionary\n",
    "filtered_words = filtered_df.columns[1:-1].tolist() # Convert the filtered columns to a list\n",
    "filtered_words = [word for word in filtered_words if len(word) > 3 or d.check(word)] # Filter out words of length 3 that are not English words\n",
    "filtered_df = df[['Label'] + filtered_words]\n",
    "spam_words = filtered_df.loc[df['Label'] == 1].iloc[:, 1:-1].sum().sort_values(ascending=False)[:10]\n",
    "print(\"\\nTop 100 words with highest frequency in spam emails:\")\n",
    "for word, count in spam_words.items():\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da1fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_spam_words = filtered_df.loc[df['Label'] == 0].iloc[:, 1:-1].sum().sort_values(ascending=False)[:10]\n",
    "print(\"\\nTop 100 words with highest frequency in non-spam emails:\")\n",
    "for word, count in not_spam_words.items():\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17833a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the unique words in each list\n",
    "unique_spam_words = set(spam_words.index) - set(not_spam_words.index)\n",
    "unique_not_spam_words = set(not_spam_words.index) - set(spam_words.index)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(list(unique_spam_words), [spam_words[word] for word in unique_spam_words])\n",
    "plt.title(\"Unique words in spam emails\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Horizontal bar chart to visualize the unique words in non-spam emails\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(list(unique_not_spam_words), [not_spam_words[word] for word in unique_not_spam_words])\n",
    "plt.title(\"Unique words in non-spam emails\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e90c4f9",
   "metadata": {},
   "source": [
    "## 4. Distribution of word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a262ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate the frequency of each word\n",
    "word_counts_unfiltered = df.iloc[:, :-1].sum().sort_values(ascending=False)\n",
    "top_10 = word_counts_unfiltered.head(10)\n",
    "for word, count in top_10.items():\n",
    "    print(f\"{word}: {count}\")\n",
    "# Create a histogram of the word frequencies\n",
    "plt.figure(figsize=(12, 6))\n",
    "sb.histplot(word_counts_unfiltered, log_scale=True, bins=50)\n",
    "plt.xlabel(\"Word Frequency\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Word Frequency Unfiltered\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_filtered = filtered_df.iloc[:, :-1].sum().sort_values(ascending=False)\n",
    "top_10 = word_counts_filtered.head(10)\n",
    "for word, count in top_10.items():\n",
    "    print(f\"{word}: {count}\")\n",
    "# Create a histogram of the word frequencies\n",
    "plt.figure(figsize=(12, 6))\n",
    "sb.histplot(word_counts_filtered, log_scale=True, bins=50)\n",
    "plt.xlabel(\"Word Frequency\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Word Frequency Filtered\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b254b03",
   "metadata": {},
   "source": [
    "## 5. Distribution of Email Length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc8df23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "email_lengths = df.iloc[:, :-1].sum(axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sb.kdeplot(email_lengths, shade=True)\n",
    "plt.xlabel(\"Email Length\")\n",
    "plt.title(\"Distribution of Email Length\")\n",
    "plt.xlim(0, 15000)\n",
    "\n",
    "# Find the peak of the density plot\n",
    "density = sb.kdeplot(email_lengths).get_lines()[0].get_data()\n",
    "x_peak = density[0][density[1].argmax()]\n",
    "y_peak = density[1].max()\n",
    "\n",
    "# Add a vertical line to the plot to indicate the peak\n",
    "plt.axvline(x_peak, color='red', linestyle='--')\n",
    "plt.text(x_peak+100, y_peak-0.00004, f\"Peak: {int(x_peak)}\", color='red', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d71e3",
   "metadata": {},
   "source": [
    "Based on the peak at 302, it appears that a significant number of emails in the dataset have a length of around 300 words. This could suggest that many emails in the dataset are concise and to the point, with fewer unnecessary or extraneous words. Additionally, the fact that the density plot has a long tail towards the right indicates that there are some emails in the dataset that are significantly longer than the majority. This could indicate that there are certain emails in the dataset that are more complex or detailed, and may require more attention or analysis. Overall, the distribution of email length suggests that there is a wide range of variability in email length in the dataset, with many emails being relatively short and concise, but with some longer and more complex emails as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce8702",
   "metadata": {},
   "source": [
    "## 6. Distribution of Email Length by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec295661",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spam_lengths = df[df['Label'] == 1].iloc[:, :-1].sum(axis=1)\n",
    "not_spam_lengths = df[df['Label'] == 0].iloc[:, :-1].sum(axis=1)\n",
    "\n",
    "# Calculate peak for each distribution\n",
    "spam_peak = spam_lengths.value_counts().idxmax()\n",
    "not_spam_peak = not_spam_lengths.value_counts().idxmax()\n",
    "\n",
    "# Print peaks\n",
    "print(\"Peak of spam email length distribution: \", spam_peak)\n",
    "print(\"Peak of not-spam email length distribution: \", not_spam_peak)\n",
    "\n",
    "# Density plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sb.kdeplot(spam_lengths, shade=True, color='red', label='Spam')\n",
    "sb.kdeplot(not_spam_lengths, shade=True, color='blue', label='Not-Spam')\n",
    "\n",
    "# Add vertical lines for peaks\n",
    "plt.axvline(spam_peak, color='red', linestyle='--', ymax=0.05)\n",
    "plt.axvline(not_spam_peak, color='blue', linestyle='--', ymax=0.05)\n",
    "plt.axvline(x=10000, color='black', linestyle='--')\n",
    "\n",
    "plt.xlim(-1000,30000)\n",
    "plt.xlabel(\"Email Length\")\n",
    "plt.title(\"Distribution of Email Length by Label\")\n",
    "plt.annotate('Spam', xy=(6000, 0.00008), color='red')\n",
    "plt.annotate('Not-Spam', xy=(6000, 0.0002), color='blue')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cf5c60",
   "metadata": {},
   "source": [
    "From the plot, we can see that the distributions of email length for spam and non-spam emails have some overlap, but there are also some noticeable differences. \n",
    "Non-spam emails generally have a higher peak and a shorter tail. In contrast, spam emails have a longer length, with a lower peak and a longer tail.\n",
    "This tells me that if email lengths are more than 10000 words,there is a higher chance that it is a spam email.\n",
    "\n",
    "We can also see that the number of non-spam emails is much higher than the number of spam emails, as indicated by the larger area under the non-spam curve. This is not surprising, as non-spam emails are typically more common in our inboxes.\n",
    "\n",
    "Overall, this plot suggests that email length could be a useful feature for distinguishing between spam and non-spam emails, as there are clear differences in the distribution of email lengths between the two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b784a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_lengths = df[df['Label'] == 1].iloc[:, :-1].sum(axis=1)\n",
    "not_spam_lengths = df[df['Label'] == 0].iloc[:, :-1].sum(axis=1)\n",
    "\n",
    "# Calculate peak for each distribution\n",
    "spam_peak = spam_lengths.value_counts().idxmax()\n",
    "not_spam_peak = not_spam_lengths.value_counts().idxmax()\n",
    "\n",
    "# Density plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sb.kdeplot(spam_lengths, shade=True, color='red', label='Spam')\n",
    "sb.kdeplot(not_spam_lengths, shade=True, color='blue', label='Not-Spam')\n",
    "\n",
    "# Add vertical lines for peaks\n",
    "plt.axvline(spam_peak, color='red', linestyle='--', ymax=0.05)\n",
    "plt.axvline(not_spam_peak, color='blue', linestyle='--', ymax=0.05)\n",
    "plt.axvline(x=10000, color='black', linestyle='--')\n",
    "\n",
    "plt.xlim(10000,30000)\n",
    "plt.ylim(0,0.00005)\n",
    "plt.xlabel(\"Email Length\")\n",
    "plt.title(\"Distribution of Email Length by Label\\n Zoomed in to see length more than 10000\")\n",
    "plt.annotate('Spam', xy=(6000, 0.00008), color='red')\n",
    "plt.annotate('Not-Spam', xy=(6000, 0.0002), color='blue')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a5aa7",
   "metadata": {},
   "source": [
    "## 7. Distribution of Email Length by Word Frequency Quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df\n",
    "df_copy['EmailLength'] = df.iloc[:, :-1].sum(axis=1)\n",
    "# Compute the total word frequency for each email\n",
    "word_freq = df.iloc[:, :-1].sum(axis=1)\n",
    "\n",
    "# Divide the emails into quartiles based on word frequency\n",
    "quartiles = pd.qcut(word_freq, q=4, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "\n",
    "# Create a new column in the DataFrame to store the quartile information\n",
    "df[\"WordFreqQuartile\"] = quartiles\n",
    "\n",
    "# Plot the distribution of email lengths within each quartile\n",
    "plt.figure(figsize=(12, 6))\n",
    "sb.kdeplot(data=df, x=\"EmailLength\", hue=\"WordFreqQuartile\")\n",
    "plt.xlabel(\"Email Length\")\n",
    "plt.title(\"Distribution of Email Length by Word Frequency Quartiles\")\n",
    "plt.xlim(0, 5000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e663e9",
   "metadata": {},
   "source": [
    " From the graph, we can see that the emails with the highest word frequency tend to have a longer length compared to those with lower word frequency.\n",
    "\n",
    "Additionally, we can see that the distribution of email lengths becomes wider and more spread out as the word frequency increases. This suggests that emails with higher word frequency are more likely to have a wider range of lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9bfa3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract words from spam emails\n",
    "spam_words = df.loc[df['Label'] == 1].iloc[:, 1:-1].sum().sort_values(ascending=False)[:50]\n",
    "spam_text = ' '.join([word for word in list(spam_words.index) if len(word) > 1 and not re.match(r'^\\W*$', word) and not re.match(r'^\\d*$', word) and word != 'EmailLength'])\n",
    "\n",
    "# Generate word cloud for spam emails\n",
    "spam_wordcloud = WordCloud(width=800, height=800, background_color='white').generate(spam_text)\n",
    "\n",
    "# Extract words from non-spam emails\n",
    "not_spam_words = df.loc[df['Label'] == 0].iloc[:, 1:-1].sum().sort_values(ascending=False)[:50]\n",
    "not_spam_text = ' '.join([word for word in list(not_spam_words.index) if len(word) > 1 and not re.match(r'^\\W*$', word) and not re.match(r'^\\d*$', word) and word != 'EmailLength'])\n",
    "\n",
    "# Generate word cloud for non-spam emails\n",
    "not_spam_wordcloud = WordCloud(width=800, height=800, background_color='white').generate(not_spam_text)\n",
    "\n",
    "# Plot the word clouds\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axs[0].imshow(spam_wordcloud, interpolation='bilinear')\n",
    "axs[0].set_title('Word Cloud for Spam Emails')\n",
    "axs[0].axis('off')\n",
    "axs[1].imshow(not_spam_wordcloud, interpolation='bilinear')\n",
    "axs[1].set_title('Word Cloud for Non-Spam Emails')\n",
    "axs[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2569f7",
   "metadata": {},
   "source": [
    "It is hard to get a meaningful insight of the wordcloud at a glance,hence we use the filtered dataframe and a smaller range of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16ef9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract words from spam emails\n",
    "spam_words = filtered_df.loc[df['Label'] == 1].iloc[:, :-1].sum().sort_values(ascending=False)[:30]\n",
    "spam_text = ' '.join(list(spam_words.index))\n",
    "unique_spam_words = set(spam_words.index) - set(not_spam_words.index)\n",
    "\n",
    "# Add unique words to the spam text\n",
    "unique_spam_text = ' '.join(list(unique_spam_words))\n",
    "spam_text += f\" {unique_spam_text}\"\n",
    "\n",
    "# Generate word cloud for spam emails\n",
    "spam_wordcloud = WordCloud(width=800, height=800, background_color='white').generate(spam_text)\n",
    "\n",
    "# Extract words from non-spam emails\n",
    "not_spam_words = filtered_df.loc[df['Label'] == 0].iloc[:, :-1].sum().sort_values(ascending=False)[:30]\n",
    "not_spam_text = ' '.join(list(not_spam_words.index))\n",
    "unique_not_spam_words = set(not_spam_words.index) - set(spam_words.index)\n",
    "\n",
    "# Add unique words to the non-spam text\n",
    "unique_not_spam_text = ' '.join(list(unique_not_spam_words))\n",
    "not_spam_text += f\" {unique_not_spam_text}\"\n",
    "\n",
    "# Generate word cloud for non-spam emails\n",
    "not_spam_wordcloud = WordCloud(width=800, height=800, background_color='white').generate(not_spam_text)\n",
    "\n",
    "# Plot the word clouds\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axs[0].imshow(spam_wordcloud, interpolation='bilinear')\n",
    "axs[0].set_title('Word Cloud for Spam Emails')\n",
    "axs[0].axis('off')\n",
    "axs[1].imshow(not_spam_wordcloud, interpolation='bilinear')\n",
    "axs[1].set_title('Word Cloud for Non-Spam Emails')\n",
    "axs[1].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf94bd",
   "metadata": {},
   "source": [
    "# Identify outlier words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69295e03",
   "metadata": {},
   "source": [
    "So far what we have been trying to visualise and see how to better identify whether an email is spam using its length and the occurence of certain words in the email.Though not as an important factor as finding the overall pattern of the frequency of words,we thought it would be good to visualise the outliers as outliers may skew statistical analysis,thus affecting the accuracy of our predictive models, and result in incorrect conclusions being drawn from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd8e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(word_counts_filtered)\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "maximum=word_counts_filtered.max()\n",
    "minimum = word_counts_filtered.min()\n",
    "print(\"Q1:\", Q1)\n",
    "print(\"Q3:\", Q3)\n",
    "print(\"IQR:\", IQR)\n",
    "print(\"Min:\",minimum)\n",
    "print(\"Max:\",maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sb.boxplot(data=word_counts_filtered, orient=\"h\", palette=\"Set2\")\n",
    "plt.title(\"Distribution of Word Frequency filtered\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "\n",
    "# Set x-axis limits to improve visualization\n",
    "plt.xlim(right=maximum)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = ((data < (Q1 - 1.5 * (Q3 - Q1))) | (data > (Q3 + 1.5 * (Q3 - Q1))))\n",
    "print(\"Number of outliers:\", rule.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee50e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_filtered = filtered_df.iloc[:, :-1].sum().sort_values(ascending=False)\n",
    "top_10 = word_counts_filtered.head(10)\n",
    "for word, count in top_10.items():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5656f8",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5e677",
   "metadata": {},
   "source": [
    "Overall,one thing that is pretty apparent when doing our data visualisation is the frequency of stop words,which are common words that are unlikely to be informative, such as `this`, `and`, and `for`.Removing these words from the data can help to reduce the dimensionality of the data and improve the algorithm's ability to learn meaningful patterns in the word frequencies.Single letters will should also be removed(will be done in data resampling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
